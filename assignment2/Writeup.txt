QUESTION 2The average validation cross entropy is 2.601QUESTION 3The cross entropy on BOTH training and validation decrease slowly with learningrate of 0.0001QUESTION 4Initializing the biases/weights to all zero with give an initial cross entropy of -log(1/250) by definition. This number is around 5.521QUESTION 5The results of Model A, B, and C are given below after training for 1 epochLearning Rate 0.001:Final Training CE 4.431Final Validation CE 4.386Final Test CE 4.393Learning Rate 0.1:Final Training CE 3.952Final Validation CE 3.303Final Test CE 3.303Learning Rate 10.0:Final Training CE 4.707Final Validation CE 4.662Final Test CE 4.668QUESTION 6Learning Rate 0.001:Final Training CE 4.379Final Validation CE 4.380Final Test CE 4.386Learning Rate 0.1:Final Training CE 2.534Final Validation CE 2.605Final Test CE 2.615Learning Rate 10.0:Final Training CE 4.665Final Validation CE 4.662Final Test CE 4.668QUESTION 7:Model A: 5 dimensional embedding, 100 dimensional hidden layerFinal Training CE 2.812Final Validation CE 2.832Final Test CE 2.835Model B: 50 dimensional embedding, 10 dimensional hidden layerFinal Training CE 3.010Final Validation CE 3.020Final Test CE 3.022Model C: 50 dimensional embedding, 200 dimensional hidden layerFinal Training CE 2.538Final Validation CE 2.607Final Test CE 2.615Model D: 100 dimensional embedding, 5 dimensional hidden layerFinal Training CE 3.225Final Validation CE 3.227Final Test CE 3.223QUESTION 9:Model A: Momentum = 0.0Final Training CE 3.990Final Validation CE 3.950Final Test CE 3.953Model B: Momentum = 0.5Final Training CE 3.332Final Validation CE 3.257Final Test CE 3.255Model C: Momentum = 0.9Final Training CE 2.713Final Validation CE 2.712Final Test CE 2.721QUESTION 10:The top 10 closest words to 'day' after training the word embedding with default parameters are 1) night2) week3) days4) years5) year6) center7) season8) game9) ago10) yesterdayQUESTION 11:The top 10 closest words to 'percent' after training the word embedding with default parameters are 1) 'including'2) '.'3) 'dr.'4) '$'5) '?'6) ')'7) 'former'8) 'university'9) 'street'10) 'general'The word doctor has index 39 and only appears 5 times in the training data. Theword 'percent' has index 99 and only appears 10 times. They appear close togetherin feature space only because they do not get updated very much as a result of training.